{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "lab-4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset and Preprocessing**\r\n",
        "### **on the ASC development dataset (2016 DCASE Challenge)**\r\n"
      ],
      "metadata": {
        "id": "zXXIfcgchJgI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#!/usr/bin/env python3\r\n",
        "import time\r\n",
        "from multiprocessing import cpu_count\r\n",
        "from typing import Union, NamedTuple\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.backends.cudnn\r\n",
        "import numpy as np\r\n",
        "from torch import nn, optim\r\n",
        "from torch.nn import functional as F\r\n",
        "import torchvision.datasets\r\n",
        "from torch.optim.optimizer import Optimizer\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torch.utils.tensorboard import SummaryWriter\r\n",
        "import torchvision\r\n",
        "from torchvision import transforms\r\n",
        "import torchaudio\r\n",
        "from torch.utils.data.dataset import Dataset\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "from IPython.display import display\r\n",
        "\r\n",
        "import argparse\r\n",
        "from pathlib import Path\r\n",
        "\r\n",
        "torch.backends.cudnn.benchmark = True\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser(\r\n",
        "    description=\"Train a simple CNN on CIFAR-10\",\r\n",
        "    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\r\n",
        ")\r\n",
        "default_dataset_dir = Path.home() / \".cache\" / \"torch\" / \"datasets\"\r\n",
        "parser.add_argument(\"--dataset-root\", default=default_dataset_dir)\r\n",
        "parser.add_argument(\"--log-dir\", default=Path(\"logs-DCASE\"), type=Path)\r\n",
        "parser.add_argument(\"--learning-rate\", default=1e-1, type=float, help=\"Learning rate\")\r\n",
        "parser.add_argument(\"--sgd-momentum\", default=0.9, type=float, help=\"SGD Momentum parameter Beta\")\r\n",
        "parser.add_argument(\r\n",
        "    \"--batch-size\",\r\n",
        "    default=64,\r\n",
        "    type=int,\r\n",
        "    help=\"Number of images within each mini-batch\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--epochs\",\r\n",
        "    default=100,\r\n",
        "    type=int,\r\n",
        "    help=\"Number of epochs (passes through the entire dataset) to train for\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--val-frequency\",\r\n",
        "    default=5,\r\n",
        "    type=int,\r\n",
        "    help=\"How frequently to test the model on the validation set in number of epochs\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--log-frequency\",\r\n",
        "    default=5,\r\n",
        "    type=int,\r\n",
        "    help=\"How frequently to save logs to tensorboard in number of steps\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--print-frequency\",\r\n",
        "    default=5,\r\n",
        "    type=int,\r\n",
        "    help=\"How frequently to print progress to the command line in number of steps\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"-j\",\r\n",
        "    \"--worker-count\",\r\n",
        "    default=cpu_count(),\r\n",
        "    type=int,\r\n",
        "    help=\"Number of worker processes used to load data.\",\r\n",
        ")\r\n",
        "parser.add_argument(\"--data-aug-hflip\", action=\"store_true\", help=\"Applies RandomHorizontalFlip\", default=False)\r\n",
        "parser.add_argument(\"--data-aug-random-order\", action=\"store_true\", help=\"Applies Transforms in a random order\", default=False)\r\n",
        "parser.add_argument(\"--data-aug-affine\", action=\"store_true\", help=\"Applies RandomAffine transform\", default=False)\r\n",
        "parser.add_argument(\r\n",
        "    \"--dropout\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Dropout probability\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data-aug-brightness\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Brightness parameter in ColorJitter transform\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data-aug-contrast\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Contrast parameter in ColorJitter transform\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data-aug-saturation\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Saturation parameter in ColorJitter transform\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data-aug-hue\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Hue parameter in ColorJitter transform\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data-aug-affine-shear\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Shear parameter in RandomAffine transform\",\r\n",
        ")\r\n",
        "parser.add_argument(\r\n",
        "    \"--data-aug-affine-degrees\",\r\n",
        "    default=0,\r\n",
        "    type=float,\r\n",
        "    help=\"Degrees parameter in RandomAffine transform\",\r\n",
        ")\r\n",
        "parser.add_argument(\"--checkpoint-path\", type=Path)\r\n",
        "parser.add_argument(\"--checkpoint-frequency\", type=int, default=1, help=\"Save a checkpoint every N epochs\")\r\n",
        "parser.add_argument(\"--resume-checkpoint\", type=Path)\r\n",
        "\r\n",
        "\r\n",
        "class DCASE(Dataset):\r\n",
        "    def __init__(self, root_dir: str, clip_duration: int):\r\n",
        "        self._root_dir = Path(root_dir)\r\n",
        "        self._labels = pd.read_csv((self._root_dir / 'labels.csv'), names=['file', 'label'])\r\n",
        "        self._labels['label'] = self._labels.label.astype('category').cat.codes.astype('int') #create categorical labels\r\n",
        "        self._clip_duration = clip_duration\r\n",
        "        self._total_duration = 30 #DCASE audio length is 30s\r\n",
        "\r\n",
        "        self._data_len = len(self._labels)\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        #reading spectrograms\r\n",
        "        filename, label = self._labels.iloc[index]\r\n",
        "        filepath = self._root_dir / 'audio'/ filename\r\n",
        "        spec = torch.from_numpy(np.load(filepath))\r\n",
        "\r\n",
        "        #splitting spec\r\n",
        "        spec = self.__trim__(spec)\r\n",
        "        return spec, label\r\n",
        "\r\n",
        "    def __trim__(self, spec: torch.Tensor) -> torch.Tensor:\r\n",
        "        \"\"\"\r\n",
        "        Trims spectrogram into multiple clips of length specified in self._num_clips\r\n",
        "        :param spec: tensor containing spectrogram of full audio signal of shape [1, 60, 1501]\r\n",
        "        :return: tensor containing stacked spectrograms of shape [num_clips, 60, clip_length] ([10, 60, 150] with 3s clips)\r\n",
        "        \"\"\"\r\n",
        "        time_steps = spec.size(-1)\r\n",
        "        self._num_clips = self._total_duration // self._clip_duration\r\n",
        "        time_interval = int(time_steps // self._num_clips)\r\n",
        "        all_clips = []\r\n",
        "        for clip_idx in range(self._num_clips):\r\n",
        "            start = clip_idx * time_interval\r\n",
        "            end = start + time_interval\r\n",
        "            spec_clip = spec[:, start:end]\r\n",
        "            #spec_clip = torch.squeeze(spec_clip)\r\n",
        "            all_clips.append(spec_clip)\r\n",
        "\r\n",
        "        specs = torch.stack(all_clips)\r\n",
        "        return specs\r\n",
        "\r\n",
        "    def get_num_clips(self) -> int:\r\n",
        "        \"\"\"\r\n",
        "        Gets number of clips the raw audio has been split into\r\n",
        "        :return: self._num_clips of type int\r\n",
        "        \"\"\"\r\n",
        "        return self._num_clips\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self._data_len\r\n",
        "    \r\n",
        "\r\n",
        "class ImageShape(NamedTuple):\r\n",
        "    height: int\r\n",
        "    width: int\r\n",
        "    channels: int\r\n",
        "\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    DEVICE = torch.device(\"cuda\")\r\n",
        "else:\r\n",
        "    DEVICE = torch.device(\"cpu\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "total_duration = 30\r\n",
        "clip_len = 3 #length of clip to visualise in seconds. Modify as you wish\r\n",
        "num_clips = total_duration // clip_len\r\n",
        "\r\n",
        "#loading data sample. Feel free to try different spectrograms\r\n",
        "spec = torch.from_numpy(np.load('data/development/audio/a017_90_120.npy'))\r\n",
        "#getting first subsection of spec determined by clip_len\r\n",
        "time_steps = spec.size(-1)\r\n",
        "time_interval = int(time_steps // num_clips)\r\n",
        "spec = spec[:, :time_interval]\r\n",
        "#Visualising spectrogram\r\n",
        "plt.imshow(spec.numpy())\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def main(args):\r\n",
        "    transform = transforms.ToTensor()\r\n",
        "    transformList = [transforms.ToTensor()]\r\n",
        "    if args.data_aug_hflip is True:\r\n",
        "        transformList.insert(0, transforms.RandomHorizontalFlip())\r\n",
        "    if args.data_aug_brightness is not 0:\r\n",
        "        transformList.insert(0, transforms.ColorJitter(brightness=args.data_aug_brightness, contrast=args.data_aug_contrast, saturation=args.data_aug_saturation, hue=args.data_aug_hue))\r\n",
        "    if args.data_aug_affine is True:\r\n",
        "        if args.data_aug_affine_shear is not 0:\r\n",
        "            transformList.insert(0, transforms.RandomAffine(degrees=args.data_aug_affine_degrees, translate=(0.1, 0.1), shear=[-args.data_aug_affine_shear, args.data_aug_affine_shear, -args.data_aug_affine_shear, args.data_aug_affine_shear]))\r\n",
        "        else:\r\n",
        "            transformList.insert(0, transforms.RandomAffine(degrees=args.data_aug_affine_degrees, translate=(0.1, 0.1)))\r\n",
        "    if len(transformList)>0:\r\n",
        "        transform = transforms.Compose([transforms.RandomOrder(transformList.remove(len(transformList)-1)), transforms.ToTensor()] if args.data_aug_random_order else transformList)\r\n",
        "    args.dataset_root.mkdir(parents=True, exist_ok=True)\r\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\r\n",
        "        args.dataset_root, train=True, download=True, transform=transform\r\n",
        "    )\r\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\r\n",
        "        args.dataset_root, train=False, download=False, transform=transform\r\n",
        "    )\r\n",
        "    data_train = DCASE('/data/development', clip_len)\r\n",
        "    data_test = DCASE('/data/evaluation', clip_len)\r\n",
        "    # Dataset tensors loaded using DCASE class - [data.__len__, num_clips, 60, clip_len]\r\n",
        "    #train_dataset = torch.empty((num_clips, 60, clip_len), dtype=torch.float32, device = 'cuda')\r\n",
        "    #test_dataset = torch.empty((num_clips, 60, clip_len), dtype=torch.float32, device = 'cuda')\r\n",
        "    #for i in data_train.__len__:\r\n",
        "    #    torch.stack(train_dataset, data_train.__getitem__(i), dim=0)\r\n",
        "    #for i in data_test.__len__:\r\n",
        "    #    torch.stack(test_dataset, data_test.__getitem__(i), dim=0)\r\n",
        "    print(data_train.__getitem__(0))\r\n",
        "    train_loader = torch.utils.data.DataLoader(\r\n",
        "        train_dataset,\r\n",
        "        shuffle=True,\r\n",
        "        batch_size=args.batch_size,\r\n",
        "        pin_memory=True,\r\n",
        "        num_workers=args.worker_count,\r\n",
        "    )\r\n",
        "    test_loader = torch.utils.data.DataLoader(\r\n",
        "        test_dataset,\r\n",
        "        shuffle=False,\r\n",
        "        batch_size=args.batch_size,\r\n",
        "        num_workers=args.worker_count,\r\n",
        "        pin_memory=True,\r\n",
        "    )\r\n",
        "\r\n",
        "    model = CNN(height=32, width=32, channels=3, class_count=10, dropout=args.dropout)\r\n",
        "    if args.resume_checkpoint.exists():\r\n",
        "            checkpoint = torch.load(args.resume_checkpoint)\r\n",
        "            print(f\"Resuming model {args.resume_checkpoint} that achieved {checkpoint['accuracy']}% accuracy\")\r\n",
        "            model.load_state_dict(checkpoint['model'])\r\n",
        "    \r\n",
        "    loss_f = nn.CrossEntropyLoss()\r\n",
        "    criterion = loss_f  #lambda logits, labels: torch.tensor(0)\r\n",
        "    ## TASK 11: Define the optimizer\r\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.sgd_momentum)\r\n",
        "\r\n",
        "    log_dir = get_summary_writer_log_dir(args)\r\n",
        "    print(f\"Writing logs to {log_dir}\")\r\n",
        "    summary_writer = SummaryWriter(\r\n",
        "            str(log_dir),\r\n",
        "            flush_secs=5\r\n",
        "    )\r\n",
        "    trainer = Trainer(\r\n",
        "        model, train_loader, test_loader, criterion, optimizer, summary_writer, DEVICE\r\n",
        "    )\r\n",
        "\r\n",
        "    trainer.train(\r\n",
        "        args.epochs,\r\n",
        "        args.val_frequency,\r\n",
        "        print_frequency=args.print_frequency,\r\n",
        "        log_frequency=args.log_frequency,\r\n",
        "    )\r\n",
        "    \r\n",
        "    summary_writer.close()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class ImageShape(NamedTuple):\r\n",
        "    height: int\r\n",
        "    width: int\r\n",
        "    channels: int\r\n",
        "\r\n",
        "class CNN(nn.Module):\r\n",
        "    def __init__(self, height: int, width: int, channels: int, class_count: int, dropout: float):\r\n",
        "        super().__init__()\r\n",
        "        self.input_shape = ImageShape(height=height, width=width, channels=channels)\r\n",
        "        self.class_count = class_count\r\n",
        "        # batch normalise input\r\n",
        "        self.bn1 = nn.BatchNorm2d(self.input_shape.channels)\r\n",
        "        self.conv1 = nn.Conv2d(\r\n",
        "            in_channels=self.input_shape.channels,\r\n",
        "            out_channels=32,\r\n",
        "            kernel_size=(5, 5),\r\n",
        "            padding=(2, 2),\r\n",
        "        )\r\n",
        "        self.initialise_layer(self.conv1)\r\n",
        "        # batch normalise conv1\r\n",
        "        self.bn2 = nn.BatchNorm2d(32)\r\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\r\n",
        "        self.conv2 = nn.Conv2d(\r\n",
        "            in_channels=32,\r\n",
        "            out_channels=64,\r\n",
        "            kernel_size=(5, 5),\r\n",
        "            padding=(2, 2),\r\n",
        "        )\r\n",
        "        self.initialise_layer(self.conv2)\r\n",
        "        # batch normalise conv2\r\n",
        "        self.bn3 = nn.BatchNorm2d(64)\r\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\r\n",
        "        self.flat1 = nn.Flatten(start_dim=1)\r\n",
        "        self.fc1 = nn.Linear(4096, 1024)\r\n",
        "        self.initialise_layer(self.fc1)\r\n",
        "        self.dropout = nn.Dropout(p=dropout)\r\n",
        "        # batch normalise fc1\r\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\r\n",
        "        self.fc2 = nn.Linear(1024, 10)\r\n",
        "        self.initialise_layer(self.fc2)\r\n",
        "\r\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\r\n",
        "        # batch normalise input\r\n",
        "        x = self.bn1(images)\r\n",
        "        x = self.conv1(images)\r\n",
        "        # batch normalise X\r\n",
        "        x = self.bn2(x)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = self.pool1(x)\r\n",
        "        x = self.conv2(x)\r\n",
        "        # batch normalise X\r\n",
        "        x = self.bn3(x)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = self.pool2(x)\r\n",
        "        x = self.flat1(x)\r\n",
        "        x = self.fc1(x)\r\n",
        "        # batch normalise X\r\n",
        "        x = self.bn4(x)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = self.dropout(x)\r\n",
        "        x = self.fc2(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def initialise_layer(layer):\r\n",
        "        if hasattr(layer, \"bias\"):\r\n",
        "            nn.init.zeros_(layer.bias)\r\n",
        "        if hasattr(layer, \"weight\"):\r\n",
        "            nn.init.kaiming_normal_(layer.weight)\r\n",
        "\r\n",
        "\r\n",
        "class Trainer:\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        model: nn.Module,\r\n",
        "        train_loader: DataLoader,\r\n",
        "        val_loader: DataLoader,\r\n",
        "        criterion: nn.Module,\r\n",
        "        optimizer: Optimizer,\r\n",
        "        summary_writer: SummaryWriter,\r\n",
        "        device: torch.device,\r\n",
        "    ):\r\n",
        "        self.model = model.to(device)\r\n",
        "        self.device = device\r\n",
        "        self.train_loader = train_loader\r\n",
        "        self.val_loader = val_loader\r\n",
        "        self.criterion = criterion\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.summary_writer = summary_writer\r\n",
        "        self.step = 0\r\n",
        "\r\n",
        "    def train(\r\n",
        "        self,\r\n",
        "        epochs: int,\r\n",
        "        val_frequency: int,\r\n",
        "        print_frequency: int = 20,\r\n",
        "        log_frequency: int = 5,\r\n",
        "        start_epoch: int = 0\r\n",
        "    ):\r\n",
        "        self.model.train()\r\n",
        "        for epoch in range(start_epoch, epochs):\r\n",
        "            self.model.train()\r\n",
        "            data_load_start_time = time.time()\r\n",
        "            for batch, labels in self.train_loader:\r\n",
        "                batch = batch.to(self.device)\r\n",
        "                labels = labels.to(self.device)\r\n",
        "                data_load_end_time = time.time()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "                logits = self.model.forward(batch)\r\n",
        "                loss = self.criterion(logits, labels)\r\n",
        "                loss.backward()\r\n",
        "                self.optimizer.step()\r\n",
        "                self.optimizer.zero_grad()\r\n",
        "\r\n",
        "                with torch.no_grad():\r\n",
        "                    preds = logits.argmax(-1)\r\n",
        "                    accuracy = compute_accuracy(labels, preds)\r\n",
        "                    class_accuracy = compute_class_accuracy(labels, preds, self.class_count)\r\n",
        "\r\n",
        "                data_load_time = data_load_end_time - data_load_start_time\r\n",
        "                step_time = time.time() - data_load_end_time\r\n",
        "                if ((self.step + 1) % log_frequency) == 0:\r\n",
        "                    self.log_metrics(epoch, accuracy, loss, data_load_time, step_time)\r\n",
        "                if ((self.step + 1) % print_frequency) == 0:\r\n",
        "                    self.print_metrics(epoch, accuracy, loss, data_load_time, step_time, class_accuracy)\r\n",
        "\r\n",
        "                self.step += 1\r\n",
        "                data_load_start_time = time.time()\r\n",
        "\r\n",
        "            self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\r\n",
        "            if ((epoch + 1) % val_frequency) == 0:\r\n",
        "                self.validate()\r\n",
        "                # self.validate() will put the model in validation mode,\r\n",
        "                # so we have to switch back to train mode afterwards\r\n",
        "                self.model.train()\r\n",
        "            if (epoch + 1) % self.args.checkpoint_frequency or (epoch + 1) == epochs:\r\n",
        "                print(f\"Saving model to {self.args.checkpoint_path}\")\r\n",
        "                torch.save({\r\n",
        "                    'args': self.args,\r\n",
        "                    'model': self.model.state_dict(),\r\n",
        "                    'accuracy': accuracy\r\n",
        "                }, self.args.checkpoint_path)\r\n",
        "\r\n",
        "    def print_metrics(self, epoch, accuracy, loss, data_load_time, step_time, class_accuracy=None):\r\n",
        "        epoch_step = self.step % len(self.train_loader)\r\n",
        "        print(\r\n",
        "                f\"epoch: [{epoch}], \"\r\n",
        "                f\"step: [{epoch_step}/{len(self.train_loader)}], \"\r\n",
        "                f\"batch loss: {loss:.5f}, \"\r\n",
        "                f\"batch accuracy: {accuracy * 100:2.2f}, \"+\r\n",
        "                (f\"class accuracies: {class_accuracy * 100:2.2f}, \" if class_accuracy is not None else \"\")+\r\n",
        "                f\"data load time: \"\r\n",
        "                f\"{data_load_time:.5f}, \"\r\n",
        "                f\"step time: {step_time:.5f}\"\r\n",
        "        )\r\n",
        "\r\n",
        "    def log_metrics(self, epoch, accuracy, loss, data_load_time, step_time):\r\n",
        "        self.summary_writer.add_scalar(\"epoch\", epoch, self.step)\r\n",
        "        self.summary_writer.add_scalars(\r\n",
        "                \"accuracy\",\r\n",
        "                {\"train\": accuracy},\r\n",
        "                self.step\r\n",
        "        )\r\n",
        "        self.summary_writer.add_scalars(\r\n",
        "                \"loss\",\r\n",
        "                {\"train\": float(loss.item())},\r\n",
        "                self.step\r\n",
        "        )\r\n",
        "        self.summary_writer.add_scalar(\r\n",
        "                \"time/data\", data_load_time, self.step\r\n",
        "        )\r\n",
        "        self.summary_writer.add_scalar(\r\n",
        "                \"time/data\", step_time, self.step\r\n",
        "        )\r\n",
        "\r\n",
        "    def validate(self):\r\n",
        "        results = {\"preds\": [], \"labels\": []}\r\n",
        "        total_loss = 0\r\n",
        "        self.model.eval()\r\n",
        "\r\n",
        "        # No need to track gradients for validation, we're not optimizing.\r\n",
        "        with torch.no_grad():\r\n",
        "            for batch, labels in self.val_loader:\r\n",
        "                batch = batch.to(self.device)\r\n",
        "                labels = labels.to(self.device)\r\n",
        "                logits = self.model(batch)\r\n",
        "                loss = self.criterion(logits, labels)\r\n",
        "                total_loss += loss.item()\r\n",
        "                preds = logits.argmax(dim=-1).cpu().numpy()\r\n",
        "                results[\"preds\"].extend(list(preds))\r\n",
        "                results[\"labels\"].extend(list(labels.cpu().numpy()))\r\n",
        "\r\n",
        "        accuracy = compute_accuracy(\r\n",
        "            np.array(results[\"labels\"]), np.array(results[\"preds\"])\r\n",
        "        )\r\n",
        "        average_loss = total_loss / len(self.val_loader)\r\n",
        "\r\n",
        "        self.summary_writer.add_scalars(\r\n",
        "                \"accuracy\",\r\n",
        "                {\"test\": accuracy},\r\n",
        "                self.step\r\n",
        "        )\r\n",
        "        self.summary_writer.add_scalars(\r\n",
        "                \"loss\",\r\n",
        "                {\"test\": average_loss},\r\n",
        "                self.step\r\n",
        "        )\r\n",
        "        print(f\"validation loss: {average_loss:.5f}, accuracy: {accuracy * 100:2.2f}\")\r\n",
        "\r\n",
        "\r\n",
        "def compute_accuracy(\r\n",
        "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray]\r\n",
        ") -> float:\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\r\n",
        "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\r\n",
        "    \"\"\"\r\n",
        "    assert len(labels) == len(preds)\r\n",
        "    return float((labels == preds).sum()) / len(labels)\r\n",
        "\r\n",
        "def compute_class_accuracy(\r\n",
        "    labels: Union[torch.Tensor, np.ndarray], preds: Union[torch.Tensor, np.ndarray], count: int\r\n",
        ") -> float:\r\n",
        "    \"\"\"\r\n",
        "    Args:\r\n",
        "        labels: ``(batch_size, class_count)`` tensor or array containing example labels\r\n",
        "        preds: ``(batch_size, class_count)`` tensor or array containing model prediction\r\n",
        "        count: number of different classes\r\n",
        "    \"\"\"\r\n",
        "    accuracies = []\r\n",
        "    assert len(labels) == len(preds)\r\n",
        "    for i in range(count):\r\n",
        "        inds = [idx for idx, element in enumerate(labels) if element == i]\r\n",
        "        inds_ = torch.Tensor(inds).int()\r\n",
        "        ls = torch.index_select(labels, 0, inds_)\r\n",
        "        ps = torch.index_select(preds, 0, inds_)\r\n",
        "        return float((ls == ps).sum()) / len(ls)\r\n",
        "\r\n",
        "def get_summary_writer_log_dir(args: argparse.Namespace) -> str:\r\n",
        "    \"\"\"Get a unique directory that hasn't been logged to before for use with a TB\r\n",
        "    SummaryWriter.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        args: CLI Arguments\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        Subdirectory of log_dir with unique subdirectory name to prevent multiple runs\r\n",
        "        from getting logged to the same TB log directory (which you can't easily\r\n",
        "        untangle in TB).\r\n",
        "    \"\"\"\r\n",
        "    tb_log_dir_prefix = (\r\n",
        "      f\"CNN_DCASE_bn_\"\r\n",
        "      f\"bs={args.batch_size}_\"\r\n",
        "      f\"lr={args.learning_rate}_\"\r\n",
        "      f\"momentum={args.sgd_momentum}_\" +\r\n",
        "      f\"brightness={args.data_aug_brightness}_\" +\r\n",
        "      (f\"saturation={args.data_aug_saturation}_\" if args.data_aug_saturation is not 0 else \"\") +\r\n",
        "      (f\"contrast={args.data_aug_contrast}_\" if args.data_aug_contrast is not 0 else \"\") +\r\n",
        "      f\"dropout={args.dropout}_\" +\r\n",
        "      (f\"hue={args.data_aug_hue}_\" if args.data_aug_hue is not 0 else \"\") +\r\n",
        "      (\"hflip_\" if args.data_aug_hflip else \"\") +\r\n",
        "      f\"run_\"\r\n",
        "    )\r\n",
        "    i = 0\r\n",
        "    while i < 1000:\r\n",
        "        tb_log_dir = args.log_dir / (tb_log_dir_prefix + str(i))\r\n",
        "        if not tb_log_dir.exists():\r\n",
        "            return str(tb_log_dir)\r\n",
        "        i += 1\r\n",
        "    return str(tb_log_dir)\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main(parser.parse_args())"
      ],
      "outputs": [],
      "metadata": {
        "id": "E1EjufH1omSl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "1LCzI3H2P3SC"
      }
    }
  ]
}